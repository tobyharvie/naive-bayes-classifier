{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d10b59ca",
   "metadata": {},
   "source": [
    "# 361 A2 Toby Harvie thar439 592248414\n",
    "## Section 1: Report\n",
    "#### Data Representation\n",
    "\n",
    "The way I decided to represent the text was to have each word/token as an attribute, with a value defined by the number of positions of that word/token in all instances of the class in the training set. Thus, I took into consideration the frequency of the word. The reasoning behind this is that the frequency of a word could definitely correlate to a class - e.g. if a certain word is repeated a lot, it could indicate a certain class rather than if it is mentioned once or twice by chance.\n",
    "\n",
    "A dictionary of attributes and frequency counts for each class was used for data representation as this allows O(1) lookup for frequency counts. Given the large number of lookups that occur in the Naive Bayes classification step, this is much faster than using other methods such as pandas table.\n",
    "\n",
    "#### Investigating Preprocessing Methods and Model Improvements\n",
    "\n",
    "A number of improvements were investigated. Those that showed improvement in the cross validation were included in the final model. I outline them all below. Most of these improvements were suggestions taken from https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf. Information was also taken from https://www.cs.cmu.edu/~tom/files/MachineLearningTomMitchell.pdf, https://www.diva-portal.org/smash/get/diva2:839705/FULLTEXT01.pdf and https://web.stanford.edu/~jurafsky/slp3/4.pdf.\n",
    "\n",
    "##### 1. (Preprocessing) Stopwords and ngrams\n",
    "For preprocessing, I removed stopwords - that is, common words such as \"the\" which should not have much influence on the classification but which could introduce bias. \n",
    "\n",
    "Another preprocessing step that I used in my final model was the use of n-grams. The idea behind this is that groups of words may infer more meaning than words on their own. For example, \"machine learning\" could be a more significant attribute that the attributes \"machine\" and \"learning\" appearing separately. Thus, I included all bigrams as attributes in the model. However, I thought that it would still be important to include unigrams, as otherwise single words that hold importance would lose their weight in the model - they would all become part of a potentially large number of different ngrams. See experiment 1 to see that the performance of the extended model decreases without the inclusion of bigrams: performance dropped to only 94% accuracy. The inclusion of trigrams was not found to increase performance.\n",
    "\n",
    "##### 2. Laplace Smoothing\n",
    "I implemented standard Laplace Smoothing, multiplying the frequency of each token $i$ by $\\frac{N_{ci}+1}{N_c+|V|}$, where $N_{ci}$ is the number of times the token appears in the class training data, $N_c$ is the total word positions in the class training data, and $|V|$ is the size of the vocabulary. Experiment 6 shows that including this has increased performance compared to the standard model (97% compared to 90%).\n",
    "\n",
    "##### 3. Hyperparameter in Laplace Smoothing\n",
    "A common adjustment to Laplace Smoothing that I read in the literature is to introduce hyperparamter $\\alpha$ to adjust the equation to $\\frac{N_{ci}+\\alpha}{N_c+\\alpha|V|}$. Such tuning could give better likelihood estimates. I experimented with varying $\\alpha$ in the extended model. Since small changes would likely overfit to the training data, I tested $\\alpha$ values with step size $0.25$. I found that $\\alpha=0.5$ gave the best improvement to the model of the values tested, with 98% accuracy. The next highest value was 0.75, with 97.22%, showing optimality of the chosen $\\alpha$ value. Furthermore, the standard Laplace smoothing which uses $\\alpha=1$ in the extended model had only 96.6%. It was important to choose this based on the extended model to account for variations caused by other extensions. See experiment 2 below. \n",
    "\n",
    "##### 4. Transforming by text frequency\n",
    "A problem with the classification as it is could be that rare words have little impact on the most probable class. However, in reality, they could be words that appear only in a certain class and thus be a good indicator. Conversely, common words may be less likely to influence the training. Thus, I implemeneted a transformation taken directly from section 4.2 of https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf, which upweights rare words and downweights common words. However, this was shown to have incredibly poor performance, getting close to 0%. Perhaps since correlation analysis has already been imlemented, this did not the desired affect.\n",
    "\n",
    "##### 5. (Preprocessing) Feature selection by correlation analysis\n",
    "I used the chi-squared test with each feature to calculate their correlation to the class labels. Sorting by their correlation, I could remove the least correlated features from the data, which reduces dimensionality and ensures that the features being used in prediction likely have an impact on classification. I chose to include only the top 100000 attributes (there were about 160000 originally) - experiment 5 shows that this is a good hyperparameter selection (giving 98% accuracy), with cutoffs below the 100000 attribute mark having lower accuracy (<98%). Experiment 4 shows that removing correlation analysis from the extended model had a noticeable decrease in performance, decreasing accuracy by about 0.5 percentage points from the extended model's performance. Among large amounts of data this is a noticeable increase which should be implemented. \n",
    "\n",
    "\n",
    "### Evaluation Procedure\n",
    "I used $k$-fold (with $k=10$) cross validation in order to evaluate the accuracy of the model using only the training data. This is a common cross validation technique which splits the data into $k$ folds. For any improvements that I considered making, I could then evaluate the model using $k$-fold evaluation before and after making the changes. See the results above and experiments below. I implemented this without any libraries in the cross_validation function in the code. In this function, the training data is randomly shuffled to ensure that the train/test split has no bias, then 10% of the training data is chosen as test data each time. The model is trained on the remaining data and then we can directly compare predictions of the synthetic data to the class labels. In this way, we have an unbiased approximation of the test data that can be used to evaluate the model.\n",
    "\n",
    "### Final Results\n",
    "The Standard Naive Bayes had an accuracy of 91.34% using cross validation and was untested on the test set.\n",
    "\n",
    "The Extended Naive Bayes had an accuracy of 98.11% using cross validation and achieved slightly better on the test set, with 98.18%. This is a significant improvement (about 7%) over the standard model and overall has a high accuracy for the given task. Given that the cross-validation accuracy and test data accuracy are very similar (in fact, the test accuracy is slightly higher), it is likely that overfitting to the training data is not occurring.\n",
    "\n",
    "The optimality of the final extended algorithm is shown by results dicussed above.\n",
    "\n",
    "# Section 2: Code and Experiments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808a53b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import csv\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "classes = ['W', 'A', 'S', 'G']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafb69ef",
   "metadata": {},
   "source": [
    "Below is the standard Naive Bayes implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20208fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class standard_naive_bayes:\n",
    "    # standard naive bayes implementation\n",
    "    def __init__(self, train_df=pd.read_csv('train.csv'), test_df=pd.read_csv('test.csv')):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.create_vocabulary()\n",
    "        self.create_freq_table()\n",
    "\n",
    "    def get_tokens(self, desc):\n",
    "        # gets tokens from a training instance\n",
    "        return desc.split()\n",
    "\n",
    "    def create_vocabulary(self):\n",
    "        # creates a set of tokens from all instances of training data\n",
    "        self.vocab = set()\n",
    "        for index, row in self.train_df.iterrows():\n",
    "            tokens = self.get_tokens(row['Description'])\n",
    "            self.vocab.update(tokens)\n",
    "\n",
    "    def create_freq_table(self):\n",
    "        # creates a table of token freqencies for each class in the training data\n",
    "\n",
    "        self.freq_table = {label : {} for label in classes}\n",
    "\n",
    "        for index, row in self.train_df.iterrows():\n",
    "            tokens = self.get_tokens(row['Description'])\n",
    "            for token in tokens:\n",
    "                # add or increment token count to frequency table\n",
    "                self.freq_table[row['Class']][token] = self.freq_table[row['Class']].get(token,0) + 1\n",
    "    \n",
    "    def get_posterior(self, label, freqs, tokens):\n",
    "        # gets the posterior probability P(label)P(w|label)\n",
    "\n",
    "        tokens = Counter(tokens)\n",
    "        # P(label). Prior probability\n",
    "        p = np.log(len(self.train_df[self.train_df['Class']==label])/len(self.train_df))\n",
    "\n",
    "        denom = sum(freqs.values())\n",
    "\n",
    "        for token, count in tokens.items():\n",
    "            # P(w|label). Likelihood\n",
    "            if token in freqs.keys():\n",
    "                p += count * np.log(freqs[token]/denom)\n",
    "            else:\n",
    "                p += count * np.log(1/denom)\n",
    "\n",
    "        return p\n",
    "\n",
    "    def classify(self, desc):\n",
    "        # classifies a new instance\n",
    "        probs = {}\n",
    "\n",
    "        for label, freqs in self.freq_table.items():\n",
    "            # iterate for each class label\n",
    "            tokens = self.get_tokens(desc)\n",
    "            probs[label] = self.get_posterior(label, freqs, tokens)\n",
    "        \n",
    "        # get the class corresponding to the maximum probability\n",
    "        return max(probs, key=probs.get)        \n",
    "\n",
    "    def predict(self, record_data=False):\n",
    "        # computes classifications for each instance in the test set and returns the predictions\n",
    "\n",
    "        preds = []\n",
    "        for index, row in self.test_df.iterrows():\n",
    "            preds.append(self.classify(row['Description']))\n",
    "\n",
    "        if record_data:\n",
    "            # write to csv\n",
    "            preds_df = pd.DataFrame({'Class':preds})\n",
    "            preds_df.index = np.arange(1, len(preds_df) + 1)\n",
    "            preds_df.to_csv('v13.csv', index_label='Id')\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b532b1",
   "metadata": {},
   "source": [
    "Below are methods used to cross validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d979ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_preds(model, cross_val_train, cross_val_test):\n",
    "    # standard method of getting predictions for the cross_validate method. Can adjust depending on experiment\n",
    "    return model(train_df=cross_val_train, test_df=cross_val_test).predict()\n",
    "\n",
    "def cross_validate(model, get_preds=get_preds, k=10, verbalize=True):\n",
    "    # we use kfold validation as discussed in the report\n",
    "\n",
    "    ins = len(train_df)\n",
    "    accuracies = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # split data\n",
    "        cross_val_train = pd.concat((train_df.iloc[:int(i*ins//k)], train_df.iloc[int((i+1)*ins//k):]))\n",
    "        cross_val_test = train_df.iloc[int(i*ins//k):int((i+1)*ins//k)]\n",
    "\n",
    "        # get predictions from model based on train/test split\n",
    "        preds = get_preds(model, cross_val_train, cross_val_test)\n",
    "\n",
    "        # compute accuracy\n",
    "        matches = sum(1 for i in range(len(cross_val_test)) if preds[i] == cross_val_test.iloc[i]['Class'])\n",
    "        accuracy = (matches / len(cross_val_test)) * 100 \n",
    "        accuracies.append( accuracy )\n",
    "        if verbalize: print(f\"Fold {i}: accuracy {accuracy}\")\n",
    "\n",
    "    print(f\"Mean accuracy: {np.mean(accuracies)}\")\n",
    "    return np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e800a90",
   "metadata": {},
   "source": [
    "We evaluate the performance of the standard model below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe99e944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: accuracy 90.9090909090909\n",
      "Fold 1: accuracy 90.68181818181819\n",
      "Fold 2: accuracy 91.5909090909091\n",
      "Fold 3: accuracy 92.5\n",
      "Fold 4: accuracy 89.77272727272727\n",
      "Fold 5: accuracy 90.22727272727272\n",
      "Fold 6: accuracy 92.04545454545455\n",
      "Fold 7: accuracy 91.81818181818183\n",
      "Fold 8: accuracy 92.5\n",
      "Fold 9: accuracy 91.36363636363637\n",
      "Mean accuracy: 91.3409090909091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91.3409090909091"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate(standard_naive_bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e0d1a0",
   "metadata": {},
   "source": [
    "Below is the extended naive bayes implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d854a053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class extended_naive_bayes(standard_naive_bayes):\n",
    "    # extended naive bayes model\n",
    "    # extensions include:\n",
    "    # including bigrams and removing stopwords\n",
    "    # attribute selection using chi-squared correlation analysis\n",
    "    # fine tuned Laplace smoothing\n",
    "    def __init__(self,train_df=pd.read_csv('train.csv'), test_df=pd.read_csv('test.csv'), record_data=False):\n",
    "        super().__init__(train_df, test_df)\n",
    "        self.cutoff=100000\n",
    "        self.attribute_selection()\n",
    "        self.record_data = record_data\n",
    "        self.alpha = 0.5\n",
    "\n",
    "    def get_tokens(self, desc):\n",
    "        # changes: implementing bigrams and removing stopwords\n",
    "        words = desc.split()\n",
    "        words = [word for word in words if word not in stopwords]\n",
    "        bigrams= [words[i-1]+'_'+words[i] for i in range(1,len(words))]\n",
    "        return words + bigrams\n",
    "    \n",
    "    def attribute_selection(self):\n",
    "        # attribute selection using chi-squared\n",
    "        # implemented using information from lectures, https://www.geeksforgeeks.org/ml-chi-square-test-for-feature-selection/ and wikipedia\n",
    "\n",
    "        # total number of token positions in each class\n",
    "        class_totals = {label: sum(freqs.values()) for label, freqs in self.freq_table.items()}\n",
    "        total_instances = sum(class_totals.values())\n",
    "\n",
    "        # total token frequency across classes\n",
    "        attr_totals = defaultdict(int)\n",
    "        for label, freqs in self.freq_table.items():\n",
    "            for token, freq in freqs.items():\n",
    "                attr_totals[token] += freq\n",
    "\n",
    "        chi_squared_scores = {}\n",
    "        for attr in attr_totals:\n",
    "            # for each attribute compute Chi-squared\n",
    "            chi_squared = 0\n",
    "            for label in self.freq_table:\n",
    "                # actual number of attribute instances in class\n",
    "                observed = self.freq_table[label].get(attr, 0)\n",
    "                # expected number of attribute instances based on ealier\n",
    "                expected = (class_totals[label] * attr_totals[attr]) / total_instances\n",
    "                if expected > 0:\n",
    "                    # basic chi squared formula\n",
    "                    chi_squared += ((observed - expected) ** 2) / expected\n",
    "            chi_squared_scores[attr] = chi_squared\n",
    "\n",
    "        # sort attributes by score\n",
    "        sorted_attrs = sorted(chi_squared_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        # select top correlating attributes\n",
    "        sorted_attrs = sorted_attrs[:self.cutoff]\n",
    "        # get just a list of attribute names\n",
    "        sorted_attrs = [x[0] for x in sorted_attrs]\n",
    "\n",
    "        # remove uncorrelated features from frequency table\n",
    "        for label in self.freq_table.keys():\n",
    "            self.freq_table[label] = {attr: self.freq_table[label][attr] for attr in sorted_attrs if attr in self.freq_table[label].keys()}\n",
    "    \n",
    "    def get_posterior(self, label, freqs, tokens):\n",
    "        # changes: Laplace smoothing. scale with parameter alpha\n",
    "\n",
    "        tokens = Counter(tokens)\n",
    "        # P(label). Prior probability\n",
    "        p = np.log(len(self.train_df[self.train_df['Class']==label])/len(self.train_df))\n",
    "\n",
    "        # denominator used in likelihood calculation\n",
    "        denom = sum(freqs.values()) + self.alpha * len(self.vocab)\n",
    "\n",
    "        # add log likelihood of each token to total probability\n",
    "        for token, count in tokens.items():\n",
    "            # P(w|label). Likelihood\n",
    "            if token in freqs.keys():\n",
    "                p += count * np.log((freqs[token] + self.alpha) / denom)\n",
    "            else:\n",
    "                p += count * np.log(1/denom)\n",
    "\n",
    "        return p\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5b3b39",
   "metadata": {},
   "source": [
    "We evaluate the performance of this model below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "18595638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: accuracy 98.4090909090909\n",
      "Fold 1: accuracy 97.95454545454545\n",
      "Fold 2: accuracy 97.72727272727273\n",
      "Fold 3: accuracy 98.4090909090909\n",
      "Fold 4: accuracy 97.27272727272728\n",
      "Fold 5: accuracy 98.18181818181819\n",
      "Fold 6: accuracy 98.4090909090909\n",
      "Fold 7: accuracy 97.95454545454545\n",
      "Fold 8: accuracy 98.4090909090909\n",
      "Fold 9: accuracy 98.4090909090909\n",
      "Mean accuracy: 98.11363636363636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98.11363636363636"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate(extended_naive_bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9e3984",
   "metadata": {},
   "source": [
    "(Experiment 1) Here we show that if we consider only unigrams on the extended model we get a lower accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53eeeff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: accuracy 94.77272727272728\n",
      "Fold 1: accuracy 93.86363636363636\n",
      "Fold 2: accuracy 94.31818181818183\n",
      "Fold 3: accuracy 95.68181818181817\n",
      "Fold 4: accuracy 92.27272727272727\n",
      "Fold 5: accuracy 92.5\n",
      "Fold 6: accuracy 93.4090909090909\n",
      "Fold 7: accuracy 93.63636363636364\n",
      "Fold 8: accuracy 95.0\n",
      "Fold 9: accuracy 94.31818181818183\n",
      "Mean accuracy: 93.97727272727273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "93.97727272727273"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class unigram_naive_bayes(extended_naive_bayes):\n",
    "    def get_tokens(self, desc):\n",
    "        # changes: removing bigrams\n",
    "        words = desc.split()\n",
    "        words = [word for word in words if word not in stopwords]\n",
    "        #bigrams= [words[i-1]+'_'+words[i] for i in range(1,len(words))]\n",
    "        return words \n",
    "\n",
    "cross_validate(unigram_naive_bayes)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f95ce40",
   "metadata": {},
   "source": [
    "(Experiment 2) The experiment below finds the best hyperparamter $\\alpha$ for the adjusted Laplaced Smoothing. We can see that $\\alpha=0.5$ produces the highest accuracy. Importantly, the common Laplace smoothing which uses $\\alpha=1$ performs fairly worse (1.5 percentage points) than choosing a different value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1dc199e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning validation for alpha=0.25\n",
      "Mean accuracy: 94.88636363636365\n",
      "Beginning validation for alpha=0.5\n",
      "Mean accuracy: 98.0\n",
      "Beginning validation for alpha=0.75\n",
      "Mean accuracy: 97.18181818181817\n",
      "Beginning validation for alpha=1.0\n",
      "Mean accuracy: 96.63636363636364\n",
      "Beginning validation for alpha=1.25\n",
      "Mean accuracy: 96.20454545454545\n"
     ]
    }
   ],
   "source": [
    "for a in [0.25*i for i in range(1,6)]:\n",
    "    # iterate through alpha values\n",
    "\n",
    "    # modified prediction function using a specified alpha value\n",
    "    def get_preds_adjusted_alpha(model, cross_val_train, cross_val_test):\n",
    "        nb = model(train_df = cross_val_train, test_df = cross_val_test, record_data=False)\n",
    "        nb.alpha = a\n",
    "        return nb.predict()\n",
    "\n",
    "    print(f\"Beginning validation for alpha={a}\")\n",
    "    cross_validate(extended_naive_bayes, get_preds_adjusted_alpha, k=5, verbalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92213e1",
   "metadata": {},
   "source": [
    "(Experiment 3) The experiment below examines the affect of transforming by text frequency as discussed in the report. It implements this transformation on the already-improved model, and we see by the cross validation results that accuracy is not improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d57ff0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: accuracy 0.0\n",
      "Fold 1: accuracy 0.0\n",
      "Fold 2: accuracy 0.0\n",
      "Fold 3: accuracy 0.0\n",
      "Fold 4: accuracy 0.0\n",
      "Fold 5: accuracy 0.0\n",
      "Fold 6: accuracy 0.0\n",
      "Fold 7: accuracy 0.0\n",
      "Fold 8: accuracy 0.0\n",
      "Fold 9: accuracy 0.22727272727272727\n",
      "Mean accuracy: 0.022727272727272728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.022727272727272728"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class nb_text_freq_transform(extended_naive_bayes):\n",
    "\n",
    "    def __init__(self,train_df=pd.read_csv('train.csv'), test_df=pd.read_csv('test.csv'), record_data=False):\n",
    "        # modified to create the instance table\n",
    "        super().__init__(train_df, test_df)\n",
    "        self.create_inst_table()\n",
    "\n",
    "    def create_inst_table(self):\n",
    "        # records number of documents in which token appears\n",
    "        self.inst_table = {}\n",
    "        for index, row in self.train_df.iterrows():\n",
    "            tokens = self.get_tokens(row['Description'])\n",
    "            for token in list(set(tokens)):\n",
    "                # add or increment token instance to table of instances\n",
    "                self.inst_table[token] = self.freq_table[row['Class']].get(token,0) + 1\n",
    "\n",
    "    def get_posterior(self, label, freqs, tokens):\n",
    "        # changes: transformation by text frequency\n",
    "\n",
    "        tokens = Counter(tokens)\n",
    "        # P(label). Prior probability\n",
    "        p = np.log(len(self.train_df[self.train_df['Class']==label])/len(self.train_df))\n",
    "\n",
    "        denom = sum(freqs.values()) + self.alpha * len(self.vocab)\n",
    "\n",
    "        for token, count in tokens.items():\n",
    "            # P(w|label). Likelihood\n",
    "            if token in freqs.keys():\n",
    "                # adjusted count function as described in the literature\n",
    "                count_adjusted = count * np.log( len(self.train_df)/self.inst_table[token])\n",
    "                p += count_adjusted * np.log((freqs[token] + self.alpha) / denom)\n",
    "            else:\n",
    "                p += count * np.log(1/denom)\n",
    "\n",
    "        return p\n",
    "    \n",
    "cross_validate(nb_text_freq_transform)                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9207d60a",
   "metadata": {},
   "source": [
    "(Experiment 4) Here we show that the use of feature selection has a marginal influence on accuracy. Interestingly, notice that some of these scores are the same as those achieved without feature selection. This could be due "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e913f0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: accuracy 98.4090909090909\n",
      "Fold 1: accuracy 97.72727272727273\n",
      "Fold 2: accuracy 97.27272727272728\n",
      "Fold 3: accuracy 98.18181818181819\n",
      "Fold 4: accuracy 97.5\n",
      "Fold 5: accuracy 98.18181818181819\n",
      "Fold 6: accuracy 98.63636363636363\n",
      "Fold 7: accuracy 97.72727272727273\n",
      "Fold 8: accuracy 98.63636363636363\n",
      "Fold 9: accuracy 98.4090909090909\n",
      "Mean accuracy: 98.06818181818183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98.06818181818183"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class naive_bayes_no_feature_selection(extended_naive_bayes):\n",
    "    \n",
    "    def attribute_selection(self):\n",
    "        # no attribute selection occurs\n",
    "        return\n",
    "    \n",
    "cross_validate(naive_bayes_no_feature_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4085db0d",
   "metadata": {},
   "source": [
    "(Experiment 5) We search for a tuned hyperparamter for how many attributes to keep after implementing the chi squared algorithm. We see that a cutoff value over 100000 have similar accuracies, perhaps indicating that there are somewhere around 100000 important attributes to include. Ultimately, I choose a cutoff 100000 for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8ae4a2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning validation for cutoff=20000\n",
      "Mean accuracy: 96.43181818181817\n",
      "Beginning validation for cutoff=60000\n",
      "Mean accuracy: 97.86363636363637\n",
      "Beginning validation for cutoff=100000\n",
      "Mean accuracy: 98.0\n",
      "Beginning validation for cutoff=140000\n",
      "Mean accuracy: 98.0\n"
     ]
    }
   ],
   "source": [
    "class nb_chi_experiment(extended_naive_bayes):\n",
    "    # removed automatic feature selection in order to preset the cutoff value\n",
    "    def __init__(self,train_df=pd.read_csv('train.csv'), test_df=pd.read_csv('test.csv'), record_data=False):\n",
    "        super().__init__(train_df, test_df)\n",
    "        self.record_data = record_data\n",
    "        self.alpha = 0.5\n",
    "\n",
    "#cross_validate(back_to_basics)\n",
    "for a in [20000, 60000, 100000, 140000]:\n",
    "\n",
    "    def get_preds_adjusted_chi(model, cross_val_train, cross_val_test):\n",
    "        nb = model(train_df = cross_val_train, test_df = cross_val_test, record_data=False)\n",
    "        nb.cutoff = a\n",
    "        nb.attribute_selection()\n",
    "        return nb.predict()\n",
    "\n",
    "    print(f\"Beginning validation for cutoff={a}\")\n",
    "    # k = 5 used to increase speed\n",
    "    cross_validate(nb_chi_experiment, get_preds_adjusted_chi, k=5, verbalize=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c606aa28",
   "metadata": {},
   "source": [
    "(Experiment 6) Here we show that the use of Laplace smoothing improved performance compared to the standard model. With only this change, we see a significant improvement from the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "29e51e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: accuracy 97.95454545454545\n",
      "Fold 1: accuracy 97.27272727272728\n",
      "Fold 2: accuracy 96.81818181818181\n",
      "Fold 3: accuracy 97.72727272727273\n",
      "Fold 4: accuracy 95.9090909090909\n",
      "Fold 5: accuracy 97.95454545454545\n",
      "Fold 6: accuracy 97.04545454545455\n",
      "Fold 7: accuracy 96.5909090909091\n",
      "Fold 8: accuracy 97.5\n",
      "Fold 9: accuracy 96.13636363636363\n",
      "Mean accuracy: 97.0909090909091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "97.0909090909091"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class nb_laplace_smoothing(standard_naive_bayes):\n",
    "    \n",
    "    def get_posterior(self, label, freqs, tokens):\n",
    "        # changes: Laplace smoothing. scale with parameter alpha\n",
    "\n",
    "        tokens = Counter(tokens)\n",
    "        # P(label). Prior probability\n",
    "        p = np.log(len(self.train_df[self.train_df['Class']==label])/len(self.train_df))\n",
    "\n",
    "        # denominator used in likelihood calculation\n",
    "        denom = sum(freqs.values()) + len(self.vocab)\n",
    "\n",
    "        for token, count in tokens.items():\n",
    "            # P(w|label). Likelihood\n",
    "            if token in freqs.keys():\n",
    "                p += count * np.log((freqs[token]) / denom)\n",
    "            else:\n",
    "                p += count * np.log(1/denom)\n",
    "\n",
    "        return p\n",
    "\n",
    "cross_validate(nb_laplace_smoothing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
